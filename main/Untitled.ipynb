{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3ddf4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = 5\n",
    "    K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + \n",
    "    K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = 5\n",
    "    K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def f1_m(precision, recall):\n",
    "    f1 = 2 * ([precision*recall]/precision+recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b59b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "import pathlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48519db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390 images found\n"
     ]
    }
   ],
   "source": [
    "#Importando imagens\n",
    "\n",
    "data_dir = pathlib.Path(\"../datasets/sign_lang/\")\n",
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "print(f\"{image_count} images found\")\n",
    "A = list(data_dir.glob('A/*'))\n",
    "B = list(data_dir.glob('B/*'))\n",
    "C = list(data_dir.glob('C/*'))\n",
    "D = list(data_dir.glob('D/*'))\n",
    "E = list(data_dir.glob('E/*'))\n",
    "F = list(data_dir.glob('F/*'))\n",
    "G = list(data_dir.glob('G/*'))\n",
    "H = list(data_dir.glob('H/*'))\n",
    "I = list(data_dir.glob('I/*'))\n",
    "J = list(data_dir.glob('J/*'))\n",
    "K = list(data_dir.glob('K/*'))\n",
    "L = list(data_dir.glob('L/*'))\n",
    "M = list(data_dir.glob('M/*'))\n",
    "N = list(data_dir.glob('N/*'))\n",
    "O = list(data_dir.glob('O/*'))\n",
    "P = list(data_dir.glob('P/*'))\n",
    "Q = list(data_dir.glob('Q/*'))\n",
    "R = list(data_dir.glob('R/*'))\n",
    "S = list(data_dir.glob('S/*'))\n",
    "T = list(data_dir.glob('T/*'))\n",
    "U = list(data_dir.glob('U/*'))\n",
    "V = list(data_dir.glob('V/*'))\n",
    "W = list(data_dir.glob('W/*'))\n",
    "X = list(data_dir.glob('X/*'))\n",
    "Y = list(data_dir.glob('Y/*'))\n",
    "Z = list(data_dir.glob('Z/*'))\n",
    "\n",
    "#Definindo algumas variáveis\n",
    "\n",
    "batch_size = 10 #total de 5 batches\n",
    "img_height = 32\n",
    "img_width = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e378029d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 390 files belonging to 26 classes.\n",
      "Using 258 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-30 11:13:57.800811: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-30 11:13:57.800848: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-30 11:13:57.800879: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (arch): /proc/driver/nvidia/version does not exist\n",
      "2022-06-30 11:13:57.815666: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 390 files belonging to 26 classes.\n",
      "Using 132 files for validation.\n"
     ]
    }
   ],
   "source": [
    "#Criando dataset de treino\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.34,\n",
    "  subset=\"training\",\n",
    "  label_mode='categorical',\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "\n",
    "#Criando dataset de test\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.34,\n",
    "  subset=\"validation\",\n",
    "  label_mode='categorical',\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "#Deninindo o número de classes\n",
    "classes = train_ds.class_names\n",
    "lenClasses = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4a03eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando algumas mudanças no dataset, para melhorar o funcionamento da rede neural\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e51d2d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "26/26 [==============================] - 5s 81ms/step - loss: 3.2126 - accuracy: 0.1163 - recall: 0.0116 - precision: 0.6000 - false_positives: 2.0000 - false_negatives: 255.0000 - true_negatives: 6448.0000 - true_positives: 3.0000 - val_loss: 3.4533 - val_accuracy: 0.0455 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_false_positives: 0.0000e+00 - val_false_negatives: 132.0000 - val_true_negatives: 3300.0000 - val_true_positives: 0.0000e+00\n",
      "Epoch 2/10\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 1.8346 - accuracy: 0.4612 - recall: 0.2132 - precision: 0.7857 - false_positives: 15.0000 - false_negatives: 203.0000 - true_negatives: 6435.0000 - true_positives: 55.0000 - val_loss: 3.2229 - val_accuracy: 0.0379 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_false_positives: 0.0000e+00 - val_false_negatives: 132.0000 - val_true_negatives: 3300.0000 - val_true_positives: 0.0000e+00\n",
      "Epoch 3/10\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 1.2077 - accuracy: 0.6357 - recall: 0.4457 - precision: 0.8582 - false_positives: 19.0000 - false_negatives: 143.0000 - true_negatives: 6431.0000 - true_positives: 115.0000 - val_loss: 2.9752 - val_accuracy: 0.1136 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_false_positives: 0.0000e+00 - val_false_negatives: 132.0000 - val_true_negatives: 3300.0000 - val_true_positives: 0.0000e+00\n",
      "Epoch 4/10\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 0.8440 - accuracy: 0.7364 - recall: 0.6202 - precision: 0.8939 - false_positives: 19.0000 - false_negatives: 98.0000 - true_negatives: 6431.0000 - true_positives: 160.0000 - val_loss: 2.7350 - val_accuracy: 0.2500 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_false_positives: 0.0000e+00 - val_false_negatives: 132.0000 - val_true_negatives: 3300.0000 - val_true_positives: 0.0000e+00\n",
      "Epoch 5/10\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.4804 - accuracy: 0.8682 - recall: 0.7674 - precision: 0.9474 - false_positives: 11.0000 - false_negatives: 60.0000 - true_negatives: 6439.0000 - true_positives: 198.0000 - val_loss: 2.3880 - val_accuracy: 0.3561 - val_recall: 0.0530 - val_precision: 1.0000 - val_false_positives: 0.0000e+00 - val_false_negatives: 125.0000 - val_true_negatives: 3300.0000 - val_true_positives: 7.0000\n",
      "Epoch 6/10\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.2542 - accuracy: 0.9419 - recall: 0.8837 - precision: 0.9744 - false_positives: 6.0000 - false_negatives: 30.0000 - true_negatives: 6444.0000 - true_positives: 228.0000 - val_loss: 2.0047 - val_accuracy: 0.5227 - val_recall: 0.0606 - val_precision: 1.0000 - val_false_positives: 0.0000e+00 - val_false_negatives: 124.0000 - val_true_negatives: 3300.0000 - val_true_positives: 8.0000\n",
      "Epoch 7/10\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 0.1314 - accuracy: 0.9806 - recall: 0.9612 - precision: 0.9880 - false_positives: 3.0000 - false_negatives: 10.0000 - true_negatives: 6447.0000 - true_positives: 248.0000 - val_loss: 1.7031 - val_accuracy: 0.6894 - val_recall: 0.0758 - val_precision: 1.0000 - val_false_positives: 0.0000e+00 - val_false_negatives: 122.0000 - val_true_negatives: 3300.0000 - val_true_positives: 10.0000\n",
      "Epoch 8/10\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.0844 - accuracy: 0.9884 - recall: 0.9806 - precision: 0.9922 - false_positives: 2.0000 - false_negatives: 5.0000 - true_negatives: 6448.0000 - true_positives: 253.0000 - val_loss: 1.5537 - val_accuracy: 0.7424 - val_recall: 0.0985 - val_precision: 1.0000 - val_false_positives: 0.0000e+00 - val_false_negatives: 119.0000 - val_true_negatives: 3300.0000 - val_true_positives: 13.0000\n",
      "Epoch 9/10\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 0.0675 - accuracy: 0.9884 - recall: 0.9845 - precision: 0.9961 - false_positives: 1.0000 - false_negatives: 4.0000 - true_negatives: 6449.0000 - true_positives: 254.0000 - val_loss: 1.4160 - val_accuracy: 0.7652 - val_recall: 0.1818 - val_precision: 1.0000 - val_false_positives: 0.0000e+00 - val_false_negatives: 108.0000 - val_true_negatives: 3300.0000 - val_true_positives: 24.0000\n",
      "Epoch 10/10\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 0.0338 - accuracy: 1.0000 - recall: 1.0000 - precision: 1.0000 - false_positives: 0.0000e+00 - false_negatives: 0.0000e+00 - true_negatives: 6450.0000 - true_positives: 258.0000 - val_loss: 1.1817 - val_accuracy: 0.8409 - val_recall: 0.3030 - val_precision: 0.9756 - val_false_positives: 1.0000 - val_false_negatives: 92.0000 - val_true_negatives: 3299.0000 - val_true_positives: 40.0000\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1.1817 - accuracy: 0.8409 - recall: 0.3030 - precision: 0.9756 - false_positives: 1.0000 - false_negatives: 92.0000 - true_negatives: 3299.0000 - true_positives: 40.0000\n",
      "Results with 10 training epochs:\n",
      "Accuracy: 0.8409090638160706\n",
      "Recall: 0.3030303120613098\n",
      "Precision: 0.9756097793579102\n"
     ]
    }
   ],
   "source": [
    "#Teste em um modelo padrão\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Rescaling(1./255))\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), padding='same', input_shape=(32, 32, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(128, (3, 3), padding='same', input_shape=(32, 32, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(256, (3, 3), padding='same', input_shape=(32, 32, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1024, activation='sigmoid'))\n",
    "model.add(tf.keras.layers.Dense(lenClasses, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='categorical_crossentropy', \n",
    "  metrics=['accuracy',\n",
    "           tf.keras.metrics.Recall(),\n",
    "           tf.keras.metrics.Precision(),\n",
    "          tf.keras.metrics.FalsePositives(),\n",
    "          tf.keras.metrics.FalseNegatives(),\n",
    "          tf.keras.metrics.TrueNegatives(),\n",
    "          tf.keras.metrics.TruePositives()])\n",
    "\n",
    "#Treinando\n",
    "Nepochs = 10\n",
    "model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=Nepochs\n",
    ")\n",
    "\n",
    "#Testando\n",
    "results = model.evaluate(val_ds)\n",
    "print(f\"Results with {Nepochs} training epochs:\")\n",
    "print(f\"Accuracy: {results[1]}\")\n",
    "print(f\"Recall: {results[2]}\")\n",
    "print(f\"Precision: {results[3]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e8d1498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(val_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
